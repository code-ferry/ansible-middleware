<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- see http://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/SecureMode.html -->

{% if namenode_ha_enable|lower == "true" %}
    <property>
        <name>dfs.nameservices</name>
        <value>ns</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.ns</name>
        <value>nn1,nn2</value>
    </property>
    <!-- nn1的rpc, http, https -->
    <property>
        <name>dfs.namenode.rpc-address.ns.nn1</name>
        <value>{{ namenode_hosts[0] }}:{{ namenode_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.ns.nn1</name>
        <value>{{ namenode_hosts[0] }}:{{ namenode_http_port }}</value>
    </property>
    <!-- nn2的rpc, http, https -->
    <property>
        <name>dfs.namenode.rpc-address.ns.nn2</name>
        <value>{{ namenode_hosts[1] }}:{{ namenode_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.ns.nn2</name>
        <value>{{ namenode_hosts[1] }}:{{ namenode_http_port }}</value>
    </property>
{% endif %}

    <!-- hdfs -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>{{ hadoop_namenode_dir }}</value>
        <description>namenode上存储hdfs名字空间元数据</description>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>{{ hadoop_datanode_dir }}</value>
        <description>datanode上数据块的物理存储位置</description>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>{{ dfs_replication }}</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>{{ dfs_permissions }}</value>
    </property>
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>{{ dfs_permissions_umask }}</value>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>{{ dfs_blocksize }}</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>{{ dfs_webhdfs_enabled }}</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>{{ dfs_acls_enabled }}</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.period</name>
        <value>600</value>
        <description>SecondaryNameNode默认每隔1分钟执行一次，改成10分钟</description>
    </property>

    <!-- kerberos to namenode -->
    <property>
        <name>dfs.block.access.token.enable</name>
        <value>{{ dfs_block_access_token_enable }}</value>
    </property>
{% if hadoop_security_authentication == "kerberos" %}
    <property>
        <name>dfs.namenode.kerberos.principal</name>
        <value>{{ hadoop_kerberos_principal_nn }}</value>
    </property>
    <property>
        <name>dfs.namenode.keytab.file</name>
        <value>{{ hadoop_keytab_file }}</value>
    </property>
    <property>
        <name>dfs.namenode.kerberos.internal.spnego.principal</name>
        <value>{{ hadoop_kerberos_principal_http }}</value>
    </property>
{% endif %}

    <!-- configuring ssl to the namenode web UI -->
    <property>
        <name>dfs.http.policy</name>
        <value>{{ dfs_http_policy }}</value>
        <description>dfs.https.enable is deprecated.</description>
    </property>
    {% if namenode_ha_enable|lower != "true" %}
    <property>
        <name>dfs.namenode.https-address</name>
        <value>0.0.0.0:{{ namenode_https_port }}</value>
        <description>This parameter is used in non-HA mode and without federation</description>
    </property>
    {% endif %}

    <!-- kerberos to secondary.namenode -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>0.0.0.0:{{ namenode_second_http_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.https-address</name>
        <value>0.0.0.0:{{ namenode_second_https_port }}</value>
    </property>
{% if hadoop_security_authentication|lower == "kerberos" %}
    <property>
        <name>dfs.secondary.namenode.keytab.file</name>
        <value>{{ hadoop_keytab_file }}</value>
    </property>
    <property>
        <name>dfs.secondary.namenode.kerberos.principal</name>
        <value>{{ hadoop_kerberos_principal_sn }}</value>
    </property>
    <property>
        <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
        <value>{{ hadoop_kerberos_principal_http }}</value>
    </property>
{% endif %}

    <!-- kerberos to datanode -->
    <property>
        <name>dfs.datanode.data.dir.perm</name>
        <value>700</value>
    </property>
    <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:{{ datanode_port }}</value>
    </property>
    <property>
        <name>dfs.datanode.ipc.address</name>
        <value>0.0.0.0:{{ datanode_ipc_port }}</value>
    </property>
    <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:{{ datanode_http_port }}</value>
    </property>
    <property>
        <name>dfs.datanode.https.address</name>
        <value>0.0.0.0:{{ datanode_https_port }}</value>
    </property>
    <property>
        <name>dfs.encrypt.data.transfer</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.data.transfer.protection</name>
        <value>integrity</value>
    </property>

{% if hadoop_security_authentication|lower == "kerberos" %}
    <property>
        <name>dfs.datanode.kerberos.principal</name>
        <value>{{ hadoop_kerberos_principal_dn }}</value>
    </property>
    <property>
        <name>dfs.datanode.keytab.file</name>
        <value>{{ hadoop_keytab_file }}</value>
    </property>
{% endif %}

    <!-- kerberos to WebHDFS -->
{% if hadoop_security_authentication|lower == "kerberos" %}
    <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>{{ hadoop_kerberos_principal_http }}</value>
    </property>
    <property>
        <name>dfs.web.authentication.kerberos.keytab</name>
        <value>{{ hadoop_keytab_file }}</value>
    </property>
{% endif %}

    <!-- kerberos to JournalNode -->
{% if namenode_ha_enable|lower == "true" %}
    <property>
        <name>dfs.journalnode.kerberos.principal</name>
        <value>{{ hadoop_kerberos_principal_jn }}</value>
    </property>
    <property>
        <name>dfs.journalnode.keytab.file</name>
        <value>{{ hadoop_keytab_file }}</value>
    </property>
    <property>
        <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
        <value>{{ hadoop_kerberos_principal_http }}</value>
    </property>
    <property>
        <name>dfs.web.authentication.kerberos.keytab</name>
        <value>{{ hadoop_keytab_file }}</value>
    </property>
    <property>
        <name>dfs.journalnode.https-address</name>
        <value>0.0.0.0:{{ journal_https_port }}</value>
    </property>
{% endif %}

    <!-- ha -->
{% if namenode_ha_enable|lower == "true" %}
    <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{{ journal_server }}/ns</value>
    </property>
    <!-- 指定JournalNode在本地磁盘存放数据的位置 -->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>{{ hadoop_journal_dir }}</value>
    </property>
    <!-- 开启NameNode失败自动切换 -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <!-- 配置失败自动切换实现方式 -->
    <property>
        <name>dfs.client.failover.proxy.provider.ns</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <!-- 配置隔离机制, 非默认22端口时可写成sshfence(hadoop:22022) -->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>
            sshfence
            shell(/bin/true)
        </value>
    </property>
    <!-- 使用隔离机制时需要ssh免登陆 -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>~/.ssh/id_rsa</value>
    </property>
{% endif %}

{% if dfs_webhdfs_enabled|lower == "true" %}
    <property>
        <name>dfs.web.ugi</name>
        <value>nn,supergroup</value>
    </property>
    <!--安全认证初始化的类-->
    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.HttpCrossOriginFilterInitializer</value>
    </property>
    <!--是否启用跨域支持-->
    <property>
        <name>hadoop.http.cross-origin.enabled</name>
        <value>true</value>
    </property>
    <!--允许跨域访问的来源，如果有多个，用逗号(,)分隔-->
    <property>
        <name>hadoop.http.cross-origin.allowed-origins</name>
        <value>*</value>
    </property>
    <!--允许跨域的方法列表，如果有多个，用逗号(,)分隔-->
    <property>
        <name>hadoop.http.cross-origin.allowed-methods</name>
        <value>GET,POST,HEAD</value>
    </property>
    <!--允许跨域的标头列表，如果有多个，用逗号(,)分隔-->
    <property>
        <name>hadoop.http.cross-origin.allowed-headers</name>
        <value>X-Requested-With,Content-Type,Accept,Origin</value>
    </property>
    <!--预检请求可以缓存的秒数-->
    <property>
        <name>hadoop.http.cross-origin.max-age</name>
        <value>1800</value>
    </property>
{% endif %}
</configuration>
