<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 指定namenode ha时，zookeeper地址 -->
    {% if namenode_ha_enable|lower == "true" %}
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://ns</value>
        <description>与hdfs-site.xml中dfs.nameservices的值要相同</description>
    </property>
    {% else %}
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{{ namenode_hosts[0] }}:{{ namenode_port }}</value>
    </property>
    {% endif %}

    <!--
        dfs.namenode.name.dir	        file://${hadoop.tmp.dir}/dfs/name
        dfs.datanode.data.dir           file://${hadoop.tmp.dir}/dfs/data
        dfs.namenode.checkpoint.dir     file://${hadoop.tmp.dir}/dfs/namesecondary
    -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:{{ hadoop_tmp_dir }}</value>
        <description>hadoop temporary directories</description>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>

    <property>
        <name>hadoop.security.authentication</name>
        <value>{{ hadoop_security_authentication }}</value>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>{{ hadoop_security_authorization }}</value>
    </property>
    <property>
        <name>hadoop.rpc.protection</name>
        <value>authentication</value>
    </property>
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[2:$1@$0](nn/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](sn/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](jn/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](dn/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](nm/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](rm/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](HTTP/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            RULE:[2:$1@$0](jhs/.*@{{ kerberos_realm_name }})s/.*/{{ install_user }}/
            DEFAULT
        </value>
    </property>

    <property>
        <name>hadoop.proxyuser.{{ install_user }}.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.{{ install_user }}.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.{{ install_user }}.users</name>
        <value>*</value>
    </property>

    <property>
        <name>hadoop.proxyuser.hive.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hive.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hive.users</name>
        <value>*</value>
    </property>

    <!-- 指定namenode ha时，zookeeper地址 -->
    {% if namenode_ha_enable|lower == "true" %}
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>{{ namenode_zk_quorum }}</value>
    </property>
    {% endif %}

    <!-- SecondaryNameNode相关配置 -->
    <property>
        <name>fs.checkpoint.period</name>
        <value>7200</value>
        <description>SecondaryNameNode默认每隔一个小时（3600秒）执行一次</description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.txns</name>
        <value>1000000</value>
        <description>SecondaryNameNode当操作次数超过100万次时执行一次</description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.check.period</name>
        <value>600</value>
        <description>NameNode默认一分钟（60）检查一次操作次数</description>
    </property>
    <property>
        <name>fs.checkpoint.size</name>
        <value>67108864</value>
        <description>设置一个edits文件大小的阈值，达到这个阈值，就强制执行一此checkpoint（即使此时没有达到period的时限），默认为64MB。</description>
    </property>

    <!-- 修改core-site.xml中的ipc参数, 防止出现连接journalnode服务ConnectException -->
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>100</value>
        <description>Indicates the number of retries a client will make to establish a server connection.</description>
    </property>
    <property>
        <name>ipc.client.connect.retry.interval</name>
        <value>10000</value>
        <description>Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.</description>
    </property>

    {% if datanode_trash_enable|lower == "true" %}
    <property>
        <name>fs.trash.interval</name>
        <value>{{ datanode_trash_interval }}</value>
    </property>
    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>{{ datanode_trash_interval }}</value>
    </property>
    {% endif %}

    {% if webui_enable|lower == "true" %}
    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
    </property>

    <!-- 定义用于HTTP web控制台的身份验证。支持的值是:simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME# -->
    <property>
        <name>hadoop.http.authentication.type</name>
        <value>simple</value>
    </property>

    <!--
    签名秘密文件，用于对身份验证令牌进行签名。
    对于集群中的每个服务，ResourceManager, NameNode, DataNode和NodeManager，应该使用不同的secret。
    这个文件应该只有运行守护进程的Unix用户可以读。
    -->
    <property>
        <name>hadoop.http.authentication.signature.secret.file</name>
        <value>{{ hadoop_home }}/hadoop-http-auth-signature-secret</value>
    </property>

    <!-- 指示在使用“简单”身份验证时是否允许匿名请求。 -->
    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>false</value>
    </property>
    {% endif %}
</configuration>
