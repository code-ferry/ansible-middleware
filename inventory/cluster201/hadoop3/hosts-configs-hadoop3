[deploy]
host201.example.com ansible_ssh_user='test' ansible_ssh_pass='test'

[hadoop]
host201.example.com ansible_ssh_user='hadoop3' ansible_ssh_pass='hadoop3' ansible_become_method=su ansible_become_user=root ansible_become_pass='oracle' namenode="yes" yarn="yes" datanode="yes"
host202.example.com ansible_ssh_user='hadoop3' ansible_ssh_pass='hadoop3' ansible_become_method=su ansible_become_user=root ansible_become_pass='oracle' datanode="yes"
host203.example.com ansible_ssh_user='hadoop3' ansible_ssh_pass='hadoop3' ansible_become_method=su ansible_become_user=root ansible_become_pass='oracle' datanode="yes"

[all:vars]
# 应用软件安装的普通用户，默认情况下是bduser。
# 因为bduser用户下已经安装hadoop2，这里安装hadoop3，需要用hadoop3用户覆盖下。
install_user=hadoop3
install_group=hadoop3
install_home="/home/hadoop3/deploy"

[hadoop:vars]
# ha开关配置
namenode_ha_enable=true
yarn_ha_enable=false

# ha相关配置
namenode_zk_quorum=10.1.8.1:2181,10.1.8.2:2181,10.1.8.3:2181
journal_port=8485
journal_https_port=8481
journal_server="{{ journal_hosts_string }}"

[hadoop:vars]
# namenode
# hadoop2.x时默认是9000
namenode_port = 8920
# hadoop2.x时默认是50070
namenode_http_port = 9870
# hadoop2.x时默认是50470
namenode_https_port = 9874
# hadoop2.x时默认是50090
namenode_second_http_port = 9868
# hadoop2.x时默认是50091
namenode_second_https_port = 9869

# datanode
# 在simple下会有冲突，改成61005。默认是61004
datanode_port = 9866
# 在simple下会有冲突，改成60022。默认是60021
datanode_ipc_port = 9867
datanode_http_port = 9864
# 在simple下会有冲突，改成50476。默认是50475
datanode_https_port = 9865

# yarn(yarn Resource Manager的服务器)
resourcemanager_port = 8032
resourcemanager_scheduler_port = 8030
resourcemanager_tracker_port = 8031
resourcemanager_admin_port = 8033
resourcemanager_webapp_port = 8088
resourcemanager_webapp_https_port = 8090

nodemanager_port = 58040
nodemanager_webapp_port = 58042
nodemanager_webapp_https_port = 58044

# mapreduce, 默认13562
mapreduce_shuffle_port = 13562

nodemanager_log_retain_seconds = 86400
nodemanager_delete_debug_delay_sec = 600

# log aggregation
log_aggr_enable = false
log_aggr_retain_seconds = 86400
log_aggr_dir = /var/container/logs

# jobhistory
jobhistory_enable = false
jobhistory_host = host201.example.com
jobhistory_port = 10020
jobhistory_webapp_port = 19888
jobhistory_intermediate_done_dir = /var/history/tmp
jobhistory_done_dir = /var/history/done
jobhistory_cache_size = 2000

[hadoop:vars]
## 通常不需要变更的变量
hadoop_version = 3.1.1
hadoop_file = "hadoop-{{ hadoop_version }}.tar.gz"
hadoop_folder = "hadoop-{{ hadoop_version }}"
hadoop_home = "{{ install_home }}/{{ hadoop_folder }}"

## 需要变更的变量
# core
hadoop_tmp_dir = "{{ hadoop_home }}/tmp"
hadoop_log_dir = "{{ hadoop_home }}/logs"
hadoop_journal_dir = "{{ hadoop_home }}/journal"
hadoop_container_conf_dir = "/etc/hadoop3"

# hdfs(单目录)
hadoop_namenode_dir = "file://{{ hadoop_tmp_dir }}/dfs/name"
hadoop_datanode_dir = "file://{{ hadoop_tmp_dir }}/dfs/data"
# hdfs(多目录)
# hadoop_namenode_dir = "file://{{ hadoop_tmp_dir }}/dfs/name1,file://{{ hadoop_tmp_dir }}/dfs/name2"
# hadoop_datanode_dir = "file://{{ hadoop_tmp_dir }}/dfs/data1,file://{{ hadoop_tmp_dir }}/dfs/data2"

[hadoop:vars]
dfs_replication = 1
dfs_permissions = true
dfs_permissions_umask = "022"
dfs_blocksize = 16777216
dfs_webhdfs_enabled = true
dfs_acls_enabled = true

# yarn
yarn_lce_group = "{{ install_group }}"
yarn_lce_user = "{{ install_user }}"
yarn_lce_limit_users = false

[hadoop:vars]
## authentication
## dfs_http_policy: HTTP_ONLY, HTTPS_ONLY, HTTP_AND_HTTPS
# kerberos: hadoop_security_authentication = kerberos
#           hadoop_security_authorization = true
#           dfs_http_policy = HTTPS_ONLY
# simple:   hadoop_security_authentication = simple
#           hadoop_security_authorization = false
#           dfs_http_policy = HTTP_ONLY

hadoop_security_authentication = kerberos
hadoop_security_authorization = true

dfs_block_access_token_enable = true
dfs_http_policy = HTTPS_ONLY
yarn_http_policy = HTTP_ONLY

# kerberos
krb5conf_path = /etc/krb5.conf

kerberos_realm_name = EXAMPLE.COM
kerberos_keytab_name = example.keytab
hadoop_keytab_path = /etc/newland/kerberos
hadoop_keytab_file = "{{ hadoop_keytab_path }}/{{ kerberos_keytab_name }}"

hadoop_kerberos_principal_fqdn = /_HOST
hadoop_kerberos_principal = {{ install_user }}{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_nn = nn{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_dn = dn{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_sn = sn{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_jn = jn{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_nm = nm{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_rm = rm{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_jhs = jhs{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}
hadoop_kerberos_principal_http = HTTP{{ hadoop_kerberos_principal_fqdn }}@{{ kerberos_realm_name }}

# ssl
hadoop_ssl_path = /etc/newland/ssl/hadoop3
hadoop_ssl_password = newland
hadoop_truststore_password = newland
hadoop_keystore_password = newland
hadoop_keystore_keypassword = newland

# node label
yarn_label_enabled = true
yarn_label_dir = /user/bduser/node-labels

# yarn
yarn_nm_memory = 8192
yarn_sc_min_memory = 512
yarn_sc_max_memory = 2048

yarn_nm_vcore = 6
yarn_sc_min_vcore = 1
yarn_sc_max_vcore = 6

# yarn cgroup
yarn_cgroup_enable = false
yarn_cgroup_hierarchy = /hadoop3-yarn
yarn_cgroup_mount = false
yarn_cgroup_mount_path = /sys/fs/cgroup
yarn_cgroup_cpu_limit = 100
yarn_cgroup_strict_resource = false
